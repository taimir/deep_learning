Training logregression with minibatches (stochastic gradient descent) leads to very good error rates and fast convergence, but the receptive fields are slightly "noisy".

Training with all of the training samples at once leads to "smoother" receptive fields, but the convergence is harder and the achieved error rates (at least in my case) were not as good.


TODO:
Do not forget to add pictures of both gradient descent with minibatches and regular gradient descent.
